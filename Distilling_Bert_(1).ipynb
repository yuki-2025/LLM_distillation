{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuki-2025/LLM_distillation/blob/main/Distilling_Bert_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd9NewHtkSh7",
        "outputId": "adc1fae0-95f0-4a60-b3bb-27624bdb3655",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from pytorch_pretrained_bert) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch_pretrained_bert) (1.26.4)\n",
            "Collecting boto3 (from pytorch_pretrained_bert)\n",
            "  Downloading boto3-1.36.17-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pytorch_pretrained_bert) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch_pretrained_bert) (4.67.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from pytorch_pretrained_bert) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Collecting botocore<1.37.0,>=1.36.17 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading botocore-1.36.17-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch_pretrained_bert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch_pretrained_bert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch_pretrained_bert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch_pretrained_bert) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.0,>=1.36.17->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.17->boto3->pytorch_pretrained_bert) (1.17.0)\n",
            "Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.36.17-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.36.17-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-nlp, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_pretrained_bert\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed boto3-1.36.17 botocore-1.36.17 jmespath-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-nlp-0.5.0 pytorch_pretrained_bert-0.6.2 s3transfer-0.11.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR1enCrY2kG1"
      },
      "source": [
        "As before, I’ll use torchnlp to load the data and the excellent PyTorch-Pretrained-BERT to build the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or4WZtptkYmv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from torchnlp.datasets import imdb_dataset\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from keras import *\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wAbpjj5kaE5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "rn.seed(321)\n",
        "np.random.seed(321)\n",
        "torch.manual_seed(321)\n",
        "torch.cuda.manual_seed(321)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePKVdHPikcvs"
      },
      "source": [
        "# Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWbxyze2kG5"
      },
      "source": [
        "### There are 25,000 reviews in the train set, we’ll use only 1000 as a labeled set and another 5,000 as an unlabeled set (I also choose only 1000 reviews from the test set to speed things up):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-3Iw9pQke54",
        "outputId": "4846d488-21e9-41d7-83f4-14585d18ab3b",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 84.1MB [00:16, 5.14MB/s]                            \n"
          ]
        }
      ],
      "source": [
        "train_data_full, test_data_full = imdb_dataset(train=True, test=True) #imdb 是default dataset\n",
        "rn.shuffle(train_data_full) #弄乱\n",
        "rn.shuffle(test_data_full)\n",
        "train_data = train_data_full[:1000] #选1000 Y-label\n",
        "test_data = test_data_full[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42mufLkP2kG6",
        "outputId": "a2d673f3-8a4c-4e45-a42e-191c87fd727b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Ah, true memories. I lived in Holland at the time and looked eagerly forward to it every Sunday evening and later Tuesdays. I saw it during my 14-16s. Very good for my (at the time school-)English, as Dutch TV provides subtitles for other languages, except for kiddies shows nowadays. So you would hear the original voices and language. - The best series were the first three ones and then after the third series, the great character, Nazi Von Gelb, who was such a formidable enemy, disappeared from the series (I don\\'t think they ever really caught him, he always escaped, leaving room to have him appear again in a next story) because evidently the series also was distributed to Germany, and a Nazi enemy wouldn\\'t go over very well! Too bad, because Geoffrey Toone did such a wonderful convincing job of portraying the intelligent Nazi aristocrat, who had this ongoing obsession to take revenge on England. It was a true delight to see this kind of high quality performance in a youth series, but Ronald Leigh-Hunt was a good counterpart and the youngsters were so normal. They were very believable to me at the time and as a kid I could just imagine to be part of these youngsters, who at the time were about four years older than me. It was a very exciting series to me, standing out in my memory of those times as a special show with \"the Prisoner\" as well. I hope they will publish a good quality DVD of the series, that would be wonderful. Even the bad copies around are still enjoyable to watch. The later series were not as good, watered down and just not as much fun as the first three. Hopefully they also find the other series with Von Gelb to be put on DVD. Greetings from Canada.',\n",
              " 'sentiment': 'pos'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "C0ocrkPK2kG7"
      },
      "source": [
        "### The first thing we do is create a baseline using logistic regression:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naBocZh6khYS",
        "outputId": "0e7b8224-235b-40c8-c1ba-3e20eb30658f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000, 1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data))) #分开 x 和 Y\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
        "\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Fs9qdkbP2kG8",
        "outputId": "89d431bd-6069-4e0f-eb78-2d1fc43ae37f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, true memories. I lived in Holland at the time and looked eagerly forward to it every Sunday evening and later Tuesdays. I saw it during my 14-16s. Very good for my (at the time school-)English, as Dutch TV provides subtitles for other languages, except for kiddies shows nowadays. So you would hear the original voices and language. - The best series were the first three ones and then after the third series, the great character, Nazi Von Gelb, who was such a formidable enemy, disappeared from the series (I don\\'t think they ever really caught him, he always escaped, leaving room to have him appear again in a next story) because evidently the series also was distributed to Germany, and a Nazi enemy wouldn\\'t go over very well! Too bad, because Geoffrey Toone did such a wonderful convincing job of portraying the intelligent Nazi aristocrat, who had this ongoing obsession to take revenge on England. It was a true delight to see this kind of high quality performance in a youth series, but Ronald Leigh-Hunt was a good counterpart and the youngsters were so normal. They were very believable to me at the time and as a kid I could just imagine to be part of these youngsters, who at the time were about four years older than me. It was a very exciting series to me, standing out in my memory of those times as a special show with \"the Prisoner\" as well. I hope they will publish a good quality DVD of the series, that would be wonderful. Even the bad copies around are still enjoyable to watch. The later series were not as good, watered down and just not as much fun as the first three. Hopefully they also find the other series with Von Gelb to be put on DVD. Greetings from Canada.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx0jDjRGkjhb",
        "outputId": "2fd57548-e3c7-4e68-caf1-9bb7a2e83f8e",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 956237.24B/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzUDfY0iklU7",
        "outputId": "e41f942d-4857-4bb6-92b3-6ae9cb2ed415",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#tokenize x features 分割词 作为input，【CLS】是开头\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
        "\n",
        "len(train_tokens), len(test_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi70p3vr2kG9",
        "outputId": "5c937101-8f72-444c-81d4-ef99df15c744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'ah',\n",
              " ',',\n",
              " 'true',\n",
              " 'memories',\n",
              " '.',\n",
              " 'i',\n",
              " 'lived',\n",
              " 'in',\n",
              " 'holland',\n",
              " 'at',\n",
              " 'the',\n",
              " 'time',\n",
              " 'and',\n",
              " 'looked',\n",
              " 'eagerly',\n",
              " 'forward',\n",
              " 'to',\n",
              " 'it',\n",
              " 'every',\n",
              " 'sunday',\n",
              " 'evening',\n",
              " 'and',\n",
              " 'later',\n",
              " 'tuesday',\n",
              " '##s',\n",
              " '.',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'it',\n",
              " 'during',\n",
              " 'my',\n",
              " '14',\n",
              " '-',\n",
              " '16',\n",
              " '##s',\n",
              " '.',\n",
              " 'very',\n",
              " 'good',\n",
              " 'for',\n",
              " 'my',\n",
              " '(',\n",
              " 'at',\n",
              " 'the',\n",
              " 'time',\n",
              " 'school',\n",
              " '-',\n",
              " ')',\n",
              " 'english',\n",
              " ',',\n",
              " 'as',\n",
              " 'dutch',\n",
              " 'tv',\n",
              " 'provides',\n",
              " 'sub',\n",
              " '##titles',\n",
              " 'for',\n",
              " 'other',\n",
              " 'languages',\n",
              " ',',\n",
              " 'except',\n",
              " 'for',\n",
              " 'kidd',\n",
              " '##ies',\n",
              " 'shows',\n",
              " 'nowadays',\n",
              " '.',\n",
              " 'so',\n",
              " 'you',\n",
              " 'would',\n",
              " 'hear',\n",
              " 'the',\n",
              " 'original',\n",
              " 'voices',\n",
              " 'and',\n",
              " 'language',\n",
              " '.',\n",
              " '-',\n",
              " 'the',\n",
              " 'best',\n",
              " 'series',\n",
              " 'were',\n",
              " 'the',\n",
              " 'first',\n",
              " 'three',\n",
              " 'ones',\n",
              " 'and',\n",
              " 'then',\n",
              " 'after',\n",
              " 'the',\n",
              " 'third',\n",
              " 'series',\n",
              " ',',\n",
              " 'the',\n",
              " 'great',\n",
              " 'character',\n",
              " ',',\n",
              " 'nazi',\n",
              " 'von',\n",
              " 'gel',\n",
              " '##b',\n",
              " ',',\n",
              " 'who',\n",
              " 'was',\n",
              " 'such',\n",
              " 'a',\n",
              " 'formidable',\n",
              " 'enemy',\n",
              " ',',\n",
              " 'disappeared',\n",
              " 'from',\n",
              " 'the',\n",
              " 'series',\n",
              " '(',\n",
              " 'i',\n",
              " 'don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'think',\n",
              " 'they',\n",
              " 'ever',\n",
              " 'really',\n",
              " 'caught',\n",
              " 'him',\n",
              " ',',\n",
              " 'he',\n",
              " 'always',\n",
              " 'escaped',\n",
              " ',',\n",
              " 'leaving',\n",
              " 'room',\n",
              " 'to',\n",
              " 'have',\n",
              " 'him',\n",
              " 'appear',\n",
              " 'again',\n",
              " 'in',\n",
              " 'a',\n",
              " 'next',\n",
              " 'story',\n",
              " ')',\n",
              " 'because',\n",
              " 'evidently',\n",
              " 'the',\n",
              " 'series',\n",
              " 'also',\n",
              " 'was',\n",
              " 'distributed',\n",
              " 'to',\n",
              " 'germany',\n",
              " ',',\n",
              " 'and',\n",
              " 'a',\n",
              " 'nazi',\n",
              " 'enemy',\n",
              " 'wouldn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'go',\n",
              " 'over',\n",
              " 'very',\n",
              " 'well',\n",
              " '!',\n",
              " 'too',\n",
              " 'bad',\n",
              " ',',\n",
              " 'because',\n",
              " 'geoffrey',\n",
              " 'too',\n",
              " '##ne',\n",
              " 'did',\n",
              " 'such',\n",
              " 'a',\n",
              " 'wonderful',\n",
              " 'convincing',\n",
              " 'job',\n",
              " 'of',\n",
              " 'portraying',\n",
              " 'the',\n",
              " 'intelligent',\n",
              " 'nazi',\n",
              " 'ari',\n",
              " '##sto',\n",
              " '##crat',\n",
              " ',',\n",
              " 'who',\n",
              " 'had',\n",
              " 'this',\n",
              " 'ongoing',\n",
              " 'obsession',\n",
              " 'to',\n",
              " 'take',\n",
              " 'revenge',\n",
              " 'on',\n",
              " 'england',\n",
              " '.',\n",
              " 'it',\n",
              " 'was',\n",
              " 'a',\n",
              " 'true',\n",
              " 'delight',\n",
              " 'to',\n",
              " 'see',\n",
              " 'this',\n",
              " 'kind',\n",
              " 'of',\n",
              " 'high',\n",
              " 'quality',\n",
              " 'performance',\n",
              " 'in',\n",
              " 'a',\n",
              " 'youth',\n",
              " 'series',\n",
              " ',',\n",
              " 'but',\n",
              " 'ronald',\n",
              " 'leigh',\n",
              " '-',\n",
              " 'hunt',\n",
              " 'was',\n",
              " 'a',\n",
              " 'good',\n",
              " 'counterpart',\n",
              " 'and',\n",
              " 'the',\n",
              " 'young',\n",
              " '##sters',\n",
              " 'were',\n",
              " 'so',\n",
              " 'normal',\n",
              " '.',\n",
              " 'they',\n",
              " 'were',\n",
              " 'very',\n",
              " 'bel',\n",
              " '##ie',\n",
              " '##vable',\n",
              " 'to',\n",
              " 'me',\n",
              " 'at',\n",
              " 'the',\n",
              " 'time',\n",
              " 'and',\n",
              " 'as',\n",
              " 'a',\n",
              " 'kid',\n",
              " 'i',\n",
              " 'could',\n",
              " 'just',\n",
              " 'imagine',\n",
              " 'to',\n",
              " 'be',\n",
              " 'part',\n",
              " 'of',\n",
              " 'these',\n",
              " 'young',\n",
              " '##sters',\n",
              " ',',\n",
              " 'who',\n",
              " 'at',\n",
              " 'the',\n",
              " 'time',\n",
              " 'were',\n",
              " 'about',\n",
              " 'four',\n",
              " 'years',\n",
              " 'older',\n",
              " 'than',\n",
              " 'me',\n",
              " '.',\n",
              " 'it',\n",
              " 'was',\n",
              " 'a',\n",
              " 'very',\n",
              " 'exciting',\n",
              " 'series',\n",
              " 'to',\n",
              " 'me',\n",
              " ',',\n",
              " 'standing',\n",
              " 'out',\n",
              " 'in',\n",
              " 'my',\n",
              " 'memory',\n",
              " 'of',\n",
              " 'those',\n",
              " 'times',\n",
              " 'as',\n",
              " 'a',\n",
              " 'special',\n",
              " 'show',\n",
              " 'with',\n",
              " '\"',\n",
              " 'the',\n",
              " 'prisoner',\n",
              " '\"',\n",
              " 'as',\n",
              " 'well',\n",
              " '.',\n",
              " 'i',\n",
              " 'hope',\n",
              " 'they',\n",
              " 'will',\n",
              " 'publish',\n",
              " 'a',\n",
              " 'good',\n",
              " 'quality',\n",
              " 'dvd',\n",
              " 'of',\n",
              " 'the',\n",
              " 'series',\n",
              " ',',\n",
              " 'that',\n",
              " 'would',\n",
              " 'be',\n",
              " 'wonderful',\n",
              " '.',\n",
              " 'even',\n",
              " 'the',\n",
              " 'bad',\n",
              " 'copies',\n",
              " 'around',\n",
              " 'are',\n",
              " 'still',\n",
              " 'enjoyable',\n",
              " 'to',\n",
              " 'watch',\n",
              " '.',\n",
              " 'the',\n",
              " 'later',\n",
              " 'series',\n",
              " 'were',\n",
              " 'not',\n",
              " 'as',\n",
              " 'good',\n",
              " ',',\n",
              " 'watered',\n",
              " 'down',\n",
              " 'and',\n",
              " 'just',\n",
              " 'not',\n",
              " 'as',\n",
              " 'much',\n",
              " 'fun',\n",
              " 'as',\n",
              " 'the',\n",
              " 'first',\n",
              " 'three',\n",
              " '.',\n",
              " 'hopefully',\n",
              " 'they',\n",
              " 'also',\n",
              " 'find',\n",
              " 'the',\n",
              " 'other',\n",
              " 'series',\n",
              " 'with',\n",
              " 'von',\n",
              " 'gel',\n",
              " '##b',\n",
              " 'to',\n",
              " 'be',\n",
              " 'put',\n",
              " 'on',\n",
              " 'dvd',\n",
              " '.',\n",
              " 'greeting',\n",
              " '##s',\n",
              " 'from',\n",
              " 'canada',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dykv7y9bkm3I",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens)) #get每个token ID\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnZDp_j4kofP",
        "outputId": "39b2bc46-1d06-4c5d-b17e-9bca22a908b4",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 512), (1000, 512))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_tokens_ids =  tf.keras.utils.pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids =  tf.keras.utils.pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "#The sequences of token IDs for both the training and test datasets are padded or truncated to a maximum length of 512.\n",
        "#Padding is adding zeros to make all sequences the same length, which is required for batch processing in many deep learning\n",
        "#frameworks. Truncation removes parts of the sequences that exceed the maximum length.\n",
        "#Both operations ensure that the input tensor has a uniform shape.\n",
        "#(token的长度填充+截断，\n",
        "#一般都是tokenizer后变成model的input\n",
        "#这里是只拿token id，别的（token_type_id, attention mask）都没有拿，[101]= CLS\n",
        "train_tokens_ids.shape, test_tokens_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWzwlPKc2kG-",
        "outputId": "0859b5a4-7675-432c-f247-c4917dbd9ce2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  101,  6289,  1010, ...,     0,     0,     0],\n",
              "       [  101,  2012,  2034, ...,     0,     0,     0],\n",
              "       [  101,  2179,  2023, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  2074, 12489, ...,     0,     0,     0],\n",
              "       [  101,  2023,  2003, ...,     0,     0,     0],\n",
              "       [  101,  2387,  2023, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_tokens_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X95BDpD2kG_",
        "outputId": "e33abc98-86ae-4f05-f345-3b6c4022571b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000,), (1000,), 0.489, 0.478)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_y = np.array(train_labels) == 'pos'\n",
        "test_y = np.array(test_labels) == 'pos'\n",
        "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)\n",
        "#The training and test labels are converted into boolean arrays where 'pos' labels are mapped to True and all others to False.\n",
        "#This suggests that it's a binary classification task with 'pos' likely standing for 'positive'.\n",
        "#The output is the shape of the training and test datasets (which should be one-dimensional arrays of labels),\n",
        "#the mean of the boolean arrays for the train and test datasets (which would give an idea of class balance),\n",
        "#and the shapes of the train and test datasets.\n",
        "#x做好了，现在做Y label, 本身是neg/pos的sentiment result， 转成array ，如果不是pos写"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge7HFFO52kG_",
        "outputId": "51c6a0db-68b6-4eaf-80d3-0207079886d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len( np.array(train_labels) == 'pos' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdm-t0b92kG_",
        "outputId": "ace82502-94b5-4f1c-f8ca-e597c472c9f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
              "       False, False, False,  True, False,  True, False, False, False,\n",
              "       False,  True, False,  True,  True,  True, False, False,  True,\n",
              "       False, False, False,  True,  True,  True, False, False, False,\n",
              "        True, False,  True, False,  True,  True,  True, False, False,\n",
              "        True, False, False,  True,  True, False, False,  True,  True,\n",
              "        True, False,  True,  True,  True,  True, False,  True, False,\n",
              "        True, False,  True, False, False, False, False,  True, False,\n",
              "        True, False,  True,  True,  True, False,  True, False,  True,\n",
              "       False, False, False,  True,  True, False, False, False, False,\n",
              "        True, False,  True,  True, False,  True, False,  True, False,\n",
              "       False])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_y[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0p2rJUokrjx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids] #每一个id都print出来找\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtwbJomY2kHA",
        "outputId": "27a26842-4f9f-4626-bcb6-3455556c3e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  101  2012  2034  1045  2134  1005  1056  2134  1005  1056  2066  2009\n",
            "  2008  2172  1010  2348  1045  2106  1012  2027  2134  1005  1056  2421\n",
            "  1996  2353  2995  3676  4245  1010  1996  2293  2090 27538  1998  7707\n",
            " 14071  2015  1005  1056  2092  4541  1998  2070  5889  2020  2205  2367\n",
            "  2013  2054  1045  2018  8078  1012  1026  7987  1013  1028  1026  7987\n",
            "  1013  1028  2101  1045  3651  2008  1010  2018  1996  3185  2042  2062\n",
            "  8884  2000  1996  2338  1010  2009  2052  2031  2042  2066  2274  2847\n",
            "  2146  1010  1998  2052  2022  2785  1997  6945  6313  1012  2085  1045\n",
            "  2066  2009  2200  2172  1010  2138  2026  8837  3494  2024  2045  2004\n",
            "  1045  6533  2098  2068  1012 10254  2428  3504  2066  2019  4850  1999\n",
            "  2444  1010  1998  2014  4955  2000  1996  2466  2012  1996 11693 11528\n",
            "  2075  1997  1996  2143  2001 10392  1010  8884  1998  2460  1012  9465\n",
            "  2485  2003 10768  6820  2721  1012  1045 15885  2014  2074  2066  2008\n",
            "  1010  2069  2025  1999 16236  2802  1996  2878  2518  1012 28517  8193\n",
            "  2003  2424  2438  1012  1045  2196  2428  4669  2010  2839  1010  1998\n",
            "  1010  2348  1045  2001  4699  1999  2032  1010  1045  6283  2032  1037\n",
            "  2843  1010  2062 11974  2043  2002  2718 10254  1012  2004  1999  1996\n",
            "  2338  1010  1996  2200  2197  2112  2003  1996  2087 10990  2028  1010\n",
            "  1998  2009  2038  2613  2381  2205  2045  1012  1026  7987  1013  1028\n",
            "  1026  7987  1013  1028  1996  3185  2003  2428  2204  1010 11974  6195\n",
            "  2008  2009  2001  1037  5861  3995  2143  2241  2588  1037  3763  2137\n",
            "  2338  1012  2049  6429  2129  2116  3297  3340  2024  2045  1010  2123\n",
            "  1005  1056  2017  2228  2061  1029     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        " for ii in train_tokens_ids[1:2] : print(ii) #positive就是0，其他的就是0 作为mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPkTLtRykvGe"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlPO0VUe2kHA"
      },
      "source": [
        "create a baseline using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BSIAyarktFR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw8Hz8d5kzv1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)\n",
        "## Train model and predict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdhLlbGek1rV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "baseline_predicted = baseline_model.predict(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QD5OuZik22r",
        "outputId": "eba43dbc-1861-44ab-ff7b-ab12be757997",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.77      0.82      0.80       522\n",
            "         pos       0.79      0.74      0.76       478\n",
            "\n",
            "    accuracy                           0.78      1000\n",
            "   macro avg       0.78      0.78      0.78      1000\n",
            "weighted avg       0.78      0.78      0.78      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels, baseline_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVpeVIPj2kHB"
      },
      "source": [
        "We get not so great results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlSiIv0Ik6KU"
      },
      "source": [
        "# BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoYdwtPo2kHB"
      },
      "source": [
        "Next step, is to fine-tune BERT, I will skip the code here, you can see it the notebook or a more detailed tutorial in my previous post. The result is a trained model called BertBinaryClassifier which uses BERT and then a linear layer to provide the pos/neg classification. The performance of this model is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v66P_LHk75D",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#This class inherits from nn.Module, which is a base class for all neural network modules in PyTorch.\n",
        "class BertBinaryClassifier(nn.Module): #建两个Model: bert和linear Regress\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        #a linear layer that maps the BERT output (which is 768-dimensional) to a single value, as it's a binary classifier.\n",
        "\n",
        "    #The forward method defines the forward pass for the input tokens and optional masks.\n",
        "    #It retrieves the pooled output from the BERT model and applies the linear layer to it.\n",
        "    #The pooled output is typically taken from the output of the BERT model's [CLS] token, which is used for classification tasks.\n",
        "\n",
        "    #把bert的output 给linear\n",
        "    def forward(self, tokens, masks=None):\n",
        "        #效果=outputs = model(tokens_tensor, segments_tensors)\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        linear_output = self.linear(pooled_output)\n",
        "        return linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2azAzV5GlBQm",
        "outputId": "e5af0900-f1f3-49f4-f0e1-f98f5caf1f71",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLguOhIKlEI6",
        "outputId": "2f94fece-f4eb-4ade-94ce-3aa12f51c5e2",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:09<00:00, 45104671.76B/s]\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_pretrained_bert/modeling.py:603: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(weights_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()\n",
        "# 定gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL6hQDdQlFhc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2sCVH_Cp2kHD"
      },
      "outputs": [],
      "source": [
        "# data111 = np.array([0, 1, 0, 1])\n",
        "\n",
        "# data111 = array([[0],\n",
        "#                  [1],\n",
        "#                  [0],\n",
        "#                  [1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qtDZfOAZlIIk",
        "outputId": "4b22d3ba-bea3-408c-b46a-b43350154ba5",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'439.065088M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#Data Preparation:\n",
        "\n",
        "train_tokens_tensor = torch.tensor(train_tokens_ids) # tensor\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float() # from 2D array to one column （-1 = 4个element自动4rows)\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks) #attention masks, which help the model distinguish between content and padding.\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "# 把y-label的tokenid,mask都放到tensor\n",
        "\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M' #CUDA memory allocated,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFT1ENZGlJba",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "# BATCH_SIZE = 4 -  during training, the model will process 4 examples at a time.\n",
        "# EPOCHS = 3 -An epoch is a complete pass through the entire training dataset. pass3次\n",
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "#dataset 打包tensor-要有tokenid,mask, actualdata(true false) 已经转成1column\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "#randomly shuffle the data, which helps in breaking any inherent ordering in the data that might affect the learning process.\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "#combines the dataset and the sampler and provides an iterable over the dataset.\n",
        "#The DataLoader can load multiple samples parallelly using multi-threading, which is highly efficient.\n",
        "#fetch batches of data with 4 samples per batch\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTM_Yg7DlU8r",
        "tags": []
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(bert_clf.parameters(), lr=3e-6)\n",
        "#An optimizer is created using the Adam algorithm, a common choice for training neural networks.\n",
        "#The learning rate is set to 3e-6, which is quite small and typical for fine-tuning BERT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkl_RvBqlfzx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "loss_func = nn.BCEWithLogitsLoss().cuda()\n",
        "# defining a loss function that is suitable for binary classification tasks, and it is preparing it to run on a CUDA-enabled GPU device.\n",
        "# from PyTorch's neural networks module (torch.nn). BCEWithLogitsLoss stands for Binary Cross-Entropy with Logits Loss.\n",
        "\n",
        "# This loss function combines a Sigmoid layer and the BCELoss (Binary Cross-Entropy Loss) in one single class.\n",
        "# This means that it's designed to operate on raw scores from the last layer of your model (logits), and it internally\n",
        "# applies the sigmoid activation function to these logits before calculating the actual binary cross-entropy loss.\n",
        "#This is more numerically stable than using a plain Sigmoid followed by a BCELoss,\n",
        "#which could result in problems due to the precision of floating-point calculations.\n",
        "\n",
        "#.cuda(): This method transfers the loss function to the GPU, allowing the computation to be done on the GPU\n",
        "# which is much faster for the operations PyTorch performs during backpropagation and gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "n3LLhs702kHL",
        "outputId": "635fd5c4-876f-496f-92c3-b60574c58894"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(tensor([[ 101, 4422, 9805,  ..., 2032, 1999, 3152],\n",
              "         [ 101, 6655, 9515,  ...,    0,    0,    0],\n",
              "         [ 101, 2205, 2919,  ...,    0,    0,    0],\n",
              "         [ 101, 1000, 2577,  ...,    0,    0,    0]], device='cuda:0'),\n",
              " tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "         [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]], device='cuda:0'))"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "for step_num, batch_data in enumerate(train_dataloader):\n",
        "        display(tuple(t.to(device) for t in batch_data) )\n",
        "        break\n",
        "        #display(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ikphGIdXlWY0",
        "outputId": "51d8f575-8f5e-4e7e-fea3-51b35c7ed168",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  3\n",
            "249/250.0 loss: 0.270366348285228 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'train_data '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[0.7006343603134155,\n",
              " 0.7020648717880249,\n",
              " 0.6997123956680298,\n",
              " 0.7282564640045166,\n",
              " 0.6846250295639038,\n",
              " 0.7019064426422119,\n",
              " 0.7087236046791077,\n",
              " 0.677876353263855,\n",
              " 0.6960886716842651,\n",
              " 0.6841288208961487,\n",
              " 0.7058299779891968,\n",
              " 0.6948597431182861,\n",
              " 0.6736465692520142,\n",
              " 0.7210195064544678,\n",
              " 0.7062524557113647,\n",
              " 0.6978850364685059,\n",
              " 0.6875633001327515,\n",
              " 0.6801782250404358,\n",
              " 0.6990731954574585,\n",
              " 0.6583489775657654,\n",
              " 0.6718583106994629,\n",
              " 0.7003213763237,\n",
              " 0.7025057077407837,\n",
              " 0.7000648975372314,\n",
              " 0.6886208057403564,\n",
              " 0.7167797684669495,\n",
              " 0.6986459493637085,\n",
              " 0.6721349358558655,\n",
              " 0.7022879719734192,\n",
              " 0.7047257423400879,\n",
              " 0.6726834774017334,\n",
              " 0.6649343967437744,\n",
              " 0.6954067349433899,\n",
              " 0.6947733759880066,\n",
              " 0.6782194375991821,\n",
              " 0.6913851499557495,\n",
              " 0.7291110157966614,\n",
              " 0.6666584014892578,\n",
              " 0.7067966461181641,\n",
              " 0.6540507674217224,\n",
              " 0.6778818368911743,\n",
              " 0.6558787822723389,\n",
              " 0.6735067963600159,\n",
              " 0.6855457425117493,\n",
              " 0.7467573881149292,\n",
              " 0.7210464477539062,\n",
              " 0.6983645558357239,\n",
              " 0.6964764595031738,\n",
              " 0.7331475615501404,\n",
              " 0.6812661290168762,\n",
              " 0.6595498323440552,\n",
              " 0.6887730360031128,\n",
              " 0.6463481187820435,\n",
              " 0.68602055311203,\n",
              " 0.6892719864845276,\n",
              " 0.6439987421035767,\n",
              " 0.6732239127159119,\n",
              " 0.6947689652442932,\n",
              " 0.6526851654052734,\n",
              " 0.6753973960876465,\n",
              " 0.6805174350738525,\n",
              " 0.6290134191513062,\n",
              " 0.676027774810791,\n",
              " 0.6145597696304321,\n",
              " 0.6351219415664673,\n",
              " 0.6591541171073914,\n",
              " 0.6696785688400269,\n",
              " 0.7331632375717163,\n",
              " 0.623985767364502,\n",
              " 0.6610211133956909,\n",
              " 0.702828586101532,\n",
              " 0.7152575850486755,\n",
              " 0.6211602687835693,\n",
              " 0.6699768304824829,\n",
              " 0.6426069736480713,\n",
              " 0.6238212585449219,\n",
              " 0.7035131454467773,\n",
              " 0.7742061614990234,\n",
              " 0.6469468474388123,\n",
              " 0.6631761789321899,\n",
              " 0.6875956654548645,\n",
              " 0.6962710618972778,\n",
              " 0.6642669439315796,\n",
              " 0.6459435820579529,\n",
              " 0.6589226126670837,\n",
              " 0.6607776880264282,\n",
              " 0.6493663191795349,\n",
              " 0.6650742292404175,\n",
              " 0.6910261511802673,\n",
              " 0.6394767165184021,\n",
              " 0.6547127366065979,\n",
              " 0.6635439395904541,\n",
              " 0.7384190559387207,\n",
              " 0.6037973165512085,\n",
              " 0.7427319288253784,\n",
              " 0.6323040723800659,\n",
              " 0.7454797029495239,\n",
              " 0.6297560930252075,\n",
              " 0.654965341091156,\n",
              " 0.6174790859222412,\n",
              " 0.6239869594573975,\n",
              " 0.745756983757019,\n",
              " 0.5447673201560974,\n",
              " 0.6433922648429871,\n",
              " 0.5898082256317139,\n",
              " 0.6296433210372925,\n",
              " 0.5797742009162903,\n",
              " 0.6890525221824646,\n",
              " 0.6766109466552734,\n",
              " 0.6485621929168701,\n",
              " 0.5989366769790649,\n",
              " 0.6140191555023193,\n",
              " 0.6014576554298401,\n",
              " 0.7047178149223328,\n",
              " 0.694902777671814,\n",
              " 0.7148953676223755,\n",
              " 0.6849820613861084,\n",
              " 0.6268088221549988,\n",
              " 0.6602036356925964,\n",
              " 0.7412185072898865,\n",
              " 0.5688952803611755,\n",
              " 0.602392315864563,\n",
              " 0.6296640634536743,\n",
              " 0.6542797088623047,\n",
              " 0.6104927062988281,\n",
              " 0.6301746368408203,\n",
              " 0.6096769571304321,\n",
              " 0.6368310451507568,\n",
              " 0.5966699123382568,\n",
              " 0.5947434306144714,\n",
              " 0.6274434328079224,\n",
              " 0.6490330100059509,\n",
              " 0.6878069639205933,\n",
              " 0.7097907066345215,\n",
              " 0.6397234201431274,\n",
              " 0.6072385311126709,\n",
              " 0.6166387796401978,\n",
              " 0.5610097050666809,\n",
              " 0.6079471707344055,\n",
              " 0.6498090028762817,\n",
              " 0.6418876051902771,\n",
              " 0.6384992599487305,\n",
              " 0.5928525328636169,\n",
              " 0.7324910163879395,\n",
              " 0.5808888673782349,\n",
              " 0.5772140026092529,\n",
              " 0.5692348480224609,\n",
              " 0.6200196743011475,\n",
              " 0.4925965368747711,\n",
              " 0.8267539739608765,\n",
              " 0.6265870332717896,\n",
              " 0.5273159146308899,\n",
              " 0.679733157157898,\n",
              " 0.5664759874343872,\n",
              " 0.6837203502655029,\n",
              " 0.5873208045959473,\n",
              " 0.5892959237098694,\n",
              " 0.5487853288650513,\n",
              " 0.5738077163696289,\n",
              " 0.5476975440979004,\n",
              " 0.5546571612358093,\n",
              " 0.5755385160446167,\n",
              " 0.5423120260238647,\n",
              " 0.581569492816925,\n",
              " 0.6721125245094299,\n",
              " 0.6445514559745789,\n",
              " 0.4863731265068054,\n",
              " 0.5926087498664856,\n",
              " 0.5981515049934387,\n",
              " 0.5856454968452454,\n",
              " 0.5873067378997803,\n",
              " 0.5649709701538086,\n",
              " 0.5365389585494995,\n",
              " 0.6701017022132874,\n",
              " 0.5774416923522949,\n",
              " 0.6492894887924194,\n",
              " 0.6224128603935242,\n",
              " 0.5941224098205566,\n",
              " 0.5295664668083191,\n",
              " 0.5951546430587769,\n",
              " 0.4788227379322052,\n",
              " 0.5569353103637695,\n",
              " 0.42297160625457764,\n",
              " 0.6453014612197876,\n",
              " 0.47769129276275635,\n",
              " 0.5502656698226929,\n",
              " 0.5044771432876587,\n",
              " 0.5429496765136719,\n",
              " 0.5260229110717773,\n",
              " 0.4768660366535187,\n",
              " 0.5138728618621826,\n",
              " 0.5336954593658447,\n",
              " 0.8981382846832275,\n",
              " 0.6281681060791016,\n",
              " 0.519656777381897,\n",
              " 0.4846314787864685,\n",
              " 0.5615243911743164,\n",
              " 0.5628019571304321,\n",
              " 0.5743798613548279,\n",
              " 0.6647111773490906,\n",
              " 0.497858464717865,\n",
              " 0.7123040556907654,\n",
              " 0.6271176338195801,\n",
              " 0.6877840757369995,\n",
              " 0.5152983665466309,\n",
              " 0.5630831718444824,\n",
              " 0.5808166265487671,\n",
              " 0.6096398830413818,\n",
              " 0.5505508780479431,\n",
              " 0.6509206295013428,\n",
              " 0.7121468782424927,\n",
              " 0.6427066326141357,\n",
              " 0.6175656914710999,\n",
              " 0.6456590890884399,\n",
              " 0.457195520401001,\n",
              " 0.603793740272522,\n",
              " 0.5979764461517334,\n",
              " 0.5836997032165527,\n",
              " 0.4839097857475281,\n",
              " 0.4251365065574646,\n",
              " 0.6993190050125122,\n",
              " 0.40851396322250366,\n",
              " 0.7539800405502319,\n",
              " 0.6092718839645386,\n",
              " 0.3789859414100647,\n",
              " 0.45991256833076477,\n",
              " 0.5373648405075073,\n",
              " 0.6881502866744995,\n",
              " 0.8021132946014404,\n",
              " 0.6546550989151001,\n",
              " 0.3958362340927124,\n",
              " 0.39507898688316345,\n",
              " 0.4369867742061615,\n",
              " 0.5633858442306519,\n",
              " 0.5895642638206482,\n",
              " 0.7703773379325867,\n",
              " 0.50282883644104,\n",
              " 0.5600112676620483,\n",
              " 0.3875165581703186,\n",
              " 0.38338419795036316,\n",
              " 0.6484459638595581,\n",
              " 0.4478070139884949,\n",
              " 0.5161174535751343,\n",
              " 0.37708109617233276,\n",
              " 0.5606570243835449,\n",
              " 0.47015854716300964,\n",
              " 0.3953457176685333,\n",
              " 0.7139452695846558,\n",
              " 0.4606609046459198,\n",
              " 0.361318439245224,\n",
              " 0.35818570852279663,\n",
              " 0.4449557662010193,\n",
              " 0.6377086043357849,\n",
              " 0.38068199157714844,\n",
              " 0.39475858211517334,\n",
              " 0.3515712022781372,\n",
              " 0.35805535316467285,\n",
              " 0.46638208627700806,\n",
              " 0.4757336378097534,\n",
              " 0.43936800956726074,\n",
              " 0.6215912699699402,\n",
              " 0.4461522400379181,\n",
              " 0.47944921255111694,\n",
              " 0.582920253276825,\n",
              " 0.4576278328895569,\n",
              " 0.4677541255950928,\n",
              " 0.6241992115974426,\n",
              " 0.3599608540534973,\n",
              " 0.7106738090515137,\n",
              " 0.32767874002456665,\n",
              " 0.30133670568466187,\n",
              " 0.31290996074676514,\n",
              " 0.45824119448661804,\n",
              " 0.6305431127548218,\n",
              " 0.3373708128929138,\n",
              " 0.5226244926452637,\n",
              " 0.5774040818214417,\n",
              " 0.4179791212081909,\n",
              " 0.32483869791030884,\n",
              " 0.5955772399902344,\n",
              " 0.37132972478866577,\n",
              " 0.3276657164096832,\n",
              " 0.47569817304611206,\n",
              " 0.3400367200374603,\n",
              " 0.46092063188552856,\n",
              " 0.5118668079376221,\n",
              " 0.3232554495334625,\n",
              " 0.31343549489974976,\n",
              " 0.41007474064826965,\n",
              " 0.318561315536499,\n",
              " 0.5026892423629761,\n",
              " 0.26190024614334106,\n",
              " 0.4684828519821167,\n",
              " 0.3143705725669861,\n",
              " 0.32389742136001587,\n",
              " 0.5996719002723694,\n",
              " 0.5494773983955383,\n",
              " 0.5241663455963135,\n",
              " 0.3858901858329773,\n",
              " 0.4188021421432495,\n",
              " 0.6407378315925598,\n",
              " 0.5618391036987305,\n",
              " 0.5251120328903198,\n",
              " 0.26413285732269287,\n",
              " 0.6657532453536987,\n",
              " 0.25464802980422974,\n",
              " 0.6467671394348145,\n",
              " 0.32206106185913086,\n",
              " 0.36210012435913086,\n",
              " 0.29934704303741455,\n",
              " 0.3708275556564331,\n",
              " 0.43801093101501465,\n",
              " 0.4123557209968567,\n",
              " 0.5432801246643066,\n",
              " 0.260823130607605,\n",
              " 0.29812294244766235,\n",
              " 0.5721986889839172,\n",
              " 0.3914843797683716,\n",
              " 0.6223229765892029,\n",
              " 0.2695912718772888,\n",
              " 0.7161146998405457,\n",
              " 0.22606444358825684,\n",
              " 0.4358273148536682,\n",
              " 0.3567228615283966,\n",
              " 0.2739415764808655,\n",
              " 0.25417548418045044,\n",
              " 0.42965102195739746,\n",
              " 0.5145727396011353,\n",
              " 0.25147318840026855,\n",
              " 0.4665873050689697,\n",
              " 0.5665821433067322,\n",
              " 0.2204008251428604,\n",
              " 0.553298830986023,\n",
              " 0.2789463698863983,\n",
              " 0.28291934728622437,\n",
              " 0.3779504597187042,\n",
              " 0.32382333278656006,\n",
              " 0.3511051833629608,\n",
              " 0.6666999459266663,\n",
              " 0.2462330013513565,\n",
              " 0.26223352551460266,\n",
              " 0.3403926491737366,\n",
              " 0.23148854076862335,\n",
              " 0.28173452615737915,\n",
              " 0.30258727073669434,\n",
              " 0.5704953670501709,\n",
              " 0.2263222634792328,\n",
              " 0.21130797266960144,\n",
              " 0.4990655481815338,\n",
              " 0.7684600949287415,\n",
              " 0.2089192271232605,\n",
              " 0.6015657186508179,\n",
              " 0.19992834329605103,\n",
              " 0.24198660254478455,\n",
              " 0.22047379612922668,\n",
              " 0.45101198554039,\n",
              " 0.20038560032844543,\n",
              " 0.6683087348937988,\n",
              " 0.47047367691993713,\n",
              " 0.3281435966491699,\n",
              " 0.17144373059272766,\n",
              " 0.24970075488090515,\n",
              " 0.655376136302948,\n",
              " 0.5410058498382568,\n",
              " 0.6304869651794434,\n",
              " 0.19870182871818542,\n",
              " 0.3253946900367737,\n",
              " 0.5072354078292847,\n",
              " 0.33719348907470703,\n",
              " 0.5329334735870361,\n",
              " 0.3801758289337158,\n",
              " 0.8208204507827759,\n",
              " 0.9539151191711426,\n",
              " 0.5512691736221313,\n",
              " 0.20264433324337006,\n",
              " 0.3424205780029297,\n",
              " 0.2869327664375305,\n",
              " 0.29039621353149414,\n",
              " 0.20765158534049988,\n",
              " 0.3717852830886841,\n",
              " 0.2425433248281479,\n",
              " 0.332369327545166,\n",
              " 0.17745059728622437,\n",
              " 0.194046288728714,\n",
              " 0.5218916535377502,\n",
              " 0.4684944450855255,\n",
              " 0.4182705879211426,\n",
              " 0.4537625312805176,\n",
              " 0.1972033977508545,\n",
              " 0.5674176216125488,\n",
              " 0.1933528035879135,\n",
              " 0.4620933532714844,\n",
              " 0.33731144666671753,\n",
              " 0.1872575581073761,\n",
              " 0.20466846227645874,\n",
              " 0.4243185520172119,\n",
              " 0.16292648017406464,\n",
              " 0.16118858754634857,\n",
              " 0.14683759212493896,\n",
              " 0.44568556547164917,\n",
              " 0.14127972722053528,\n",
              " 0.17817217111587524,\n",
              " 0.2551118731498718,\n",
              " 0.5718655586242676,\n",
              " 0.5129415988922119,\n",
              " 0.5142805576324463,\n",
              " 0.11963256448507309,\n",
              " 0.12557660043239594,\n",
              " 0.1379932314157486,\n",
              " 0.12646234035491943,\n",
              " 0.13685961067676544,\n",
              " 0.28190475702285767,\n",
              " 0.5519856810569763,\n",
              " 0.6142771244049072,\n",
              " 0.6366549730300903,\n",
              " 0.24643251299858093,\n",
              " 0.4001784026622772,\n",
              " 0.13147951662540436,\n",
              " 0.3644790053367615,\n",
              " 0.35790276527404785,\n",
              " 0.1930800974369049,\n",
              " 0.5287529230117798,\n",
              " 0.13660985231399536,\n",
              " 0.120757557451725,\n",
              " 0.23583842813968658,\n",
              " 0.15445533394813538,\n",
              " 0.11724944412708282,\n",
              " 0.17453530430793762,\n",
              " 0.7201825976371765,\n",
              " 0.24273937940597534,\n",
              " 0.19307860732078552,\n",
              " 0.25916755199432373,\n",
              " 0.09797246009111404,\n",
              " 0.7607473134994507,\n",
              " 0.42300498485565186,\n",
              " 0.17525725066661835,\n",
              " 0.6490439176559448,\n",
              " 0.5166555643081665,\n",
              " 0.19647914171218872,\n",
              " 0.12191202491521835,\n",
              " 0.7307364344596863,\n",
              " 0.6457301378250122,\n",
              " 0.10344135761260986,\n",
              " 0.09265744686126709,\n",
              " 0.11500643193721771,\n",
              " 0.32772672176361084,\n",
              " 0.0938640609383583,\n",
              " 0.14261244237422943,\n",
              " 0.11897440254688263,\n",
              " 0.875281810760498,\n",
              " 0.5699308514595032,\n",
              " 0.7057104110717773,\n",
              " 0.11975298821926117,\n",
              " 0.1194496899843216,\n",
              " 0.16434518992900848,\n",
              " 0.1799681931734085,\n",
              " 0.2644737958908081,\n",
              " 0.8599314093589783,\n",
              " 0.0887366235256195,\n",
              " 0.682916522026062,\n",
              " 0.12338908761739731,\n",
              " 0.6017253398895264,\n",
              " 0.0813569724559784,\n",
              " 0.10327818244695663,\n",
              " 0.12234102934598923,\n",
              " 0.980167806148529,\n",
              " 0.5275420546531677,\n",
              " 0.6351075768470764,\n",
              " 0.12985272705554962,\n",
              " 0.5041060447692871,\n",
              " 0.2547118067741394,\n",
              " 0.15007099509239197,\n",
              " 0.7918477654457092,\n",
              " 0.17044493556022644,\n",
              " 0.42482078075408936,\n",
              " 0.5734084248542786,\n",
              " 0.10893328487873077,\n",
              " 0.08453167229890823,\n",
              " 0.25145643949508667,\n",
              " 0.08415277302265167,\n",
              " 0.22501742839813232,\n",
              " 0.1514642834663391,\n",
              " 0.10221613943576813,\n",
              " 0.07547950744628906,\n",
              " 0.11533025652170181,\n",
              " 0.7716858386993408,\n",
              " 0.09149788320064545,\n",
              " 0.08922012150287628,\n",
              " 1.1663801670074463,\n",
              " 0.11023591458797455,\n",
              " 0.1996692419052124,\n",
              " 0.07052909582853317,\n",
              " 0.2777831554412842,\n",
              " 0.5257450342178345,\n",
              " 0.2918643355369568,\n",
              " 0.08969345688819885,\n",
              " 0.0937318429350853,\n",
              " 0.14690732955932617,\n",
              " 0.1337071657180786,\n",
              " 0.17298981547355652,\n",
              " 0.318106472492218,\n",
              " 0.20018580555915833,\n",
              " 0.6719282865524292,\n",
              " 0.0673477053642273,\n",
              " 1.1340436935424805,\n",
              " 0.062115542590618134,\n",
              " 0.12216293066740036,\n",
              " 0.643589973449707,\n",
              " 1.5001795291900635,\n",
              " 0.7716066837310791,\n",
              " 0.06435316056013107,\n",
              " 0.08887944370508194,\n",
              " 0.32154130935668945,\n",
              " 0.05867951363325119,\n",
              " 1.202841877937317,\n",
              " 0.08945894241333008,\n",
              " 0.07228539884090424,\n",
              " 0.07011719793081284,\n",
              " 0.5630063414573669,\n",
              " 0.05539023131132126,\n",
              " 0.7750856280326843,\n",
              " 0.07058188319206238,\n",
              " 0.11804673075675964,\n",
              " 0.13894817233085632,\n",
              " 0.3403274118900299,\n",
              " 0.05617154762148857,\n",
              " 0.7028883695602417,\n",
              " 0.06431322544813156,\n",
              " 0.5509001016616821,\n",
              " 0.08463748544454575,\n",
              " 0.08828423917293549,\n",
              " 0.05765755847096443,\n",
              " 0.14776726067066193,\n",
              " 0.08666910976171494,\n",
              " 0.1651577353477478,\n",
              " 0.057234469801187515,\n",
              " 0.1179904043674469,\n",
              " 0.044642675668001175,\n",
              " 0.7682733535766602,\n",
              " 0.08280433714389801,\n",
              " 0.06769722700119019,\n",
              " 0.05987272039055824,\n",
              " 0.10838169604539871,\n",
              " 0.7345170974731445,\n",
              " 0.32641229033470154,\n",
              " 0.04263727739453316,\n",
              " 0.3062068223953247,\n",
              " 0.8979693055152893,\n",
              " 0.06086738407611847,\n",
              " 0.045106541365385056,\n",
              " 0.0492539182305336,\n",
              " 0.0518445149064064,\n",
              " 0.05519731715321541,\n",
              " 0.07573945820331573,\n",
              " 0.044126737862825394,\n",
              " 0.04449981451034546,\n",
              " 0.03637759014964104,\n",
              " 0.03796081244945526,\n",
              " 0.1222991868853569,\n",
              " 0.09596413373947144,\n",
              " 0.8396771550178528,\n",
              " 0.03679618984460831,\n",
              " 0.045326896011829376,\n",
              " 0.0375078022480011,\n",
              " 0.8441492319107056,\n",
              " 0.04014023765921593,\n",
              " 0.835034191608429,\n",
              " 0.16826938092708588,\n",
              " 0.5095193982124329,\n",
              " 0.7547645568847656,\n",
              " 0.0452759675681591,\n",
              " 0.935665488243103,\n",
              " 0.037701576948165894,\n",
              " 0.3481435179710388,\n",
              " 0.7221517562866211,\n",
              " 0.03827042132616043,\n",
              " 0.11412142217159271,\n",
              " 1.0900388956069946,\n",
              " 0.036865413188934326,\n",
              " 0.0372033566236496,\n",
              " 0.03028760850429535,\n",
              " 0.050748005509376526,\n",
              " 0.03368094563484192,\n",
              " 0.030823159962892532,\n",
              " 0.033175013959407806,\n",
              " 0.04317682981491089,\n",
              " 0.030900541692972183,\n",
              " 0.18302088975906372,\n",
              " 0.02613738551735878,\n",
              " 0.05313166230916977,\n",
              " 0.03706534951925278,\n",
              " 0.9181287884712219,\n",
              " 0.037370022386312485,\n",
              " 0.5787395238876343,\n",
              " 0.03246953338384628,\n",
              " 0.030309204012155533,\n",
              " 1.9385945796966553,\n",
              " 0.9043400883674622,\n",
              " 0.02998332679271698,\n",
              " 0.030304627493023872,\n",
              " 0.7874543070793152,\n",
              " 0.5188353061676025,\n",
              " 0.026478659361600876,\n",
              " 0.0388764813542366,\n",
              " 0.9141373038291931,\n",
              " 0.05816102400422096,\n",
              " 0.09016919881105423,\n",
              " 0.9408053159713745,\n",
              " 0.09629438072443008,\n",
              " 0.7293000221252441,\n",
              " 0.050171300768852234,\n",
              " 0.02510767988860607,\n",
              " 1.669219732284546,\n",
              " 0.8627539277076721,\n",
              " 0.04979889839887619,\n",
              " 0.02275925502181053,\n",
              " 0.034282438457012177,\n",
              " 1.1962348222732544,\n",
              " 0.12018213421106339,\n",
              " 0.03230662643909454,\n",
              " 0.025457819923758507,\n",
              " 0.0323689803481102,\n",
              " 0.028302839025855064,\n",
              " 0.08680898696184158,\n",
              " 0.038488250225782394,\n",
              " 0.03534110635519028,\n",
              " 0.03305415064096451,\n",
              " 0.02567606046795845,\n",
              " 0.0270998477935791,\n",
              " 0.033570848405361176,\n",
              " 0.7521411776542664,\n",
              " 0.4931912422180176,\n",
              " 0.038132134824991226,\n",
              " 0.1494092345237732,\n",
              " 0.06213020533323288,\n",
              " 0.027545928955078125,\n",
              " 0.0651012733578682,\n",
              " 0.024486936628818512,\n",
              " 0.028948724269866943,\n",
              " 0.8544381856918335,\n",
              " 0.18164847791194916,\n",
              " 0.9696035385131836,\n",
              " 0.035725101828575134,\n",
              " 0.8179830312728882,\n",
              " 0.90842604637146,\n",
              " 0.06512553989887238,\n",
              " 0.08896072953939438,\n",
              " 0.026070833206176758,\n",
              " 0.3815426826477051,\n",
              " 0.0293373242020607,\n",
              " 0.02056010626256466,\n",
              " 0.024872541427612305,\n",
              " 0.02174431085586548,\n",
              " 0.026755910366773605,\n",
              " 1.0655949115753174,\n",
              " 0.022724637761712074,\n",
              " 0.414901465177536,\n",
              " 0.9673186540603638,\n",
              " 0.7402475476264954,\n",
              " 0.01846073940396309,\n",
              " 0.01912444829940796,\n",
              " 0.019897978752851486,\n",
              " 0.032500430941581726,\n",
              " 0.01856459304690361,\n",
              " 0.2202739715576172,\n",
              " 0.10833123326301575,\n",
              " 0.04633280634880066,\n",
              " 0.04166233539581299,\n",
              " 0.39140620827674866,\n",
              " 0.034278810024261475,\n",
              " 0.024913035333156586,\n",
              " 0.5124971866607666,\n",
              " 0.027166573330760002,\n",
              " 0.02978779934346676,\n",
              " 0.018198436126112938,\n",
              " 0.020195484161376953,\n",
              " 0.20668068528175354,\n",
              " 0.07518650591373444,\n",
              " 0.0168845783919096,\n",
              " 0.03226054459810257,\n",
              " 0.2135460525751114,\n",
              " 0.03950190916657448,\n",
              " 0.01603783294558525,\n",
              " 0.01739683747291565,\n",
              " 0.46147051453590393,\n",
              " 0.018225172534585,\n",
              " 0.7732803225517273,\n",
              " 0.017593927681446075,\n",
              " 0.0158919095993042,\n",
              " 0.021485134959220886,\n",
              " 0.016662169247865677,\n",
              " 0.6421076655387878,\n",
              " 0.9420518279075623,\n",
              " 0.37747183442115784,\n",
              " 0.018170766532421112,\n",
              " 0.018779752776026726,\n",
              " 0.025531254708766937,\n",
              " 0.026440244168043137,\n",
              " 0.01641196385025978,\n",
              " 0.02384175918996334,\n",
              " 0.0436251126229763,\n",
              " 0.047990284860134125,\n",
              " 0.01590770296752453,\n",
              " 0.698979377746582,\n",
              " 1.0894719362258911,\n",
              " 0.015836888924241066,\n",
              " 1.135180950164795,\n",
              " 0.016539881005883217,\n",
              " 0.01896577514708042,\n",
              " 0.022698352113366127,\n",
              " 0.01899748668074608,\n",
              " 1.4184808731079102,\n",
              " 1.0251659154891968,\n",
              " 1.0604130029678345,\n",
              " 0.6242427229881287,\n",
              " 0.013770895078778267,\n",
              " 0.34905633330345154,\n",
              " 0.0808669924736023,\n",
              " 1.061785340309143,\n",
              " 0.022556215524673462,\n",
              " 0.016418378800153732,\n",
              " 0.013015813194215298,\n",
              " 0.019933287054300308,\n",
              " 0.021749509498476982,\n",
              " 0.020206838846206665,\n",
              " 0.01412195060402155,\n",
              " 0.0147333275526762,\n",
              " 0.0129624605178833,\n",
              " 0.012440187856554985,\n",
              " 0.015275764279067516,\n",
              " 0.016293466091156006,\n",
              " 0.045504871755838394,\n",
              " 0.013321549631655216,\n",
              " 0.8988396525382996,\n",
              " 1.1130450963974,\n",
              " 0.9448889493942261,\n",
              " 0.014128267765045166,\n",
              " 0.08842289447784424,\n",
              " 0.012302147224545479,\n",
              " 0.014937815256416798,\n",
              " 1.0613102912902832,\n",
              " 0.012903185561299324,\n",
              " 0.012807277031242847,\n",
              " 0.01267687976360321,\n",
              " 0.3733433485031128,\n",
              " 1.1051151752471924,\n",
              " 0.018236804753541946,\n",
              " 0.011887408792972565,\n",
              " 0.01280332263559103,\n",
              " 0.8886162042617798]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'steps '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 100,\n",
              " 101,\n",
              " 102,\n",
              " 103,\n",
              " 104,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 108,\n",
              " 109,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 118,\n",
              " 119,\n",
              " 120,\n",
              " 121,\n",
              " 122,\n",
              " 123,\n",
              " 124,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128,\n",
              " 129,\n",
              " 130,\n",
              " 131,\n",
              " 132,\n",
              " 133,\n",
              " 134,\n",
              " 135,\n",
              " 136,\n",
              " 137,\n",
              " 138,\n",
              " 139,\n",
              " 140,\n",
              " 141,\n",
              " 142,\n",
              " 143,\n",
              " 144,\n",
              " 145,\n",
              " 146,\n",
              " 147,\n",
              " 148,\n",
              " 149,\n",
              " 150,\n",
              " 151,\n",
              " 152,\n",
              " 153,\n",
              " 154,\n",
              " 155,\n",
              " 156,\n",
              " 157,\n",
              " 158,\n",
              " 159,\n",
              " 160,\n",
              " 161,\n",
              " 162,\n",
              " 163,\n",
              " 164,\n",
              " 165,\n",
              " 166,\n",
              " 167,\n",
              " 168,\n",
              " 169,\n",
              " 170,\n",
              " 171,\n",
              " 172,\n",
              " 173,\n",
              " 174,\n",
              " 175,\n",
              " 176,\n",
              " 177,\n",
              " 178,\n",
              " 179,\n",
              " 180,\n",
              " 181,\n",
              " 182,\n",
              " 183,\n",
              " 184,\n",
              " 185,\n",
              " 186,\n",
              " 187,\n",
              " 188,\n",
              " 189,\n",
              " 190,\n",
              " 191,\n",
              " 192,\n",
              " 193,\n",
              " 194,\n",
              " 195,\n",
              " 196,\n",
              " 197,\n",
              " 198,\n",
              " 199,\n",
              " 200,\n",
              " 201,\n",
              " 202,\n",
              " 203,\n",
              " 204,\n",
              " 205,\n",
              " 206,\n",
              " 207,\n",
              " 208,\n",
              " 209,\n",
              " 210,\n",
              " 211,\n",
              " 212,\n",
              " 213,\n",
              " 214,\n",
              " 215,\n",
              " 216,\n",
              " 217,\n",
              " 218,\n",
              " 219,\n",
              " 220,\n",
              " 221,\n",
              " 222,\n",
              " 223,\n",
              " 224,\n",
              " 225,\n",
              " 226,\n",
              " 227,\n",
              " 228,\n",
              " 229,\n",
              " 230,\n",
              " 231,\n",
              " 232,\n",
              " 233,\n",
              " 234,\n",
              " 235,\n",
              " 236,\n",
              " 237,\n",
              " 238,\n",
              " 239,\n",
              " 240,\n",
              " 241,\n",
              " 242,\n",
              " 243,\n",
              " 244,\n",
              " 245,\n",
              " 246,\n",
              " 247,\n",
              " 248,\n",
              " 249,\n",
              " 250,\n",
              " 251,\n",
              " 252,\n",
              " 253,\n",
              " 254,\n",
              " 255,\n",
              " 256,\n",
              " 257,\n",
              " 258,\n",
              " 259,\n",
              " 260,\n",
              " 261,\n",
              " 262,\n",
              " 263,\n",
              " 264,\n",
              " 265,\n",
              " 266,\n",
              " 267,\n",
              " 268,\n",
              " 269,\n",
              " 270,\n",
              " 271,\n",
              " 272,\n",
              " 273,\n",
              " 274,\n",
              " 275,\n",
              " 276,\n",
              " 277,\n",
              " 278,\n",
              " 279,\n",
              " 280,\n",
              " 281,\n",
              " 282,\n",
              " 283,\n",
              " 284,\n",
              " 285,\n",
              " 286,\n",
              " 287,\n",
              " 288,\n",
              " 289,\n",
              " 290,\n",
              " 291,\n",
              " 292,\n",
              " 293,\n",
              " 294,\n",
              " 295,\n",
              " 296,\n",
              " 297,\n",
              " 298,\n",
              " 299,\n",
              " 300,\n",
              " 301,\n",
              " 302,\n",
              " 303,\n",
              " 304,\n",
              " 305,\n",
              " 306,\n",
              " 307,\n",
              " 308,\n",
              " 309,\n",
              " 310,\n",
              " 311,\n",
              " 312,\n",
              " 313,\n",
              " 314,\n",
              " 315,\n",
              " 316,\n",
              " 317,\n",
              " 318,\n",
              " 319,\n",
              " 320,\n",
              " 321,\n",
              " 322,\n",
              " 323,\n",
              " 324,\n",
              " 325,\n",
              " 326,\n",
              " 327,\n",
              " 328,\n",
              " 329,\n",
              " 330,\n",
              " 331,\n",
              " 332,\n",
              " 333,\n",
              " 334,\n",
              " 335,\n",
              " 336,\n",
              " 337,\n",
              " 338,\n",
              " 339,\n",
              " 340,\n",
              " 341,\n",
              " 342,\n",
              " 343,\n",
              " 344,\n",
              " 345,\n",
              " 346,\n",
              " 347,\n",
              " 348,\n",
              " 349,\n",
              " 350,\n",
              " 351,\n",
              " 352,\n",
              " 353,\n",
              " 354,\n",
              " 355,\n",
              " 356,\n",
              " 357,\n",
              " 358,\n",
              " 359,\n",
              " 360,\n",
              " 361,\n",
              " 362,\n",
              " 363,\n",
              " 364,\n",
              " 365,\n",
              " 366,\n",
              " 367,\n",
              " 368,\n",
              " 369,\n",
              " 370,\n",
              " 371,\n",
              " 372,\n",
              " 373,\n",
              " 374,\n",
              " 375,\n",
              " 376,\n",
              " 377,\n",
              " 378,\n",
              " 379,\n",
              " 380,\n",
              " 381,\n",
              " 382,\n",
              " 383,\n",
              " 384,\n",
              " 385,\n",
              " 386,\n",
              " 387,\n",
              " 388,\n",
              " 389,\n",
              " 390,\n",
              " 391,\n",
              " 392,\n",
              " 393,\n",
              " 394,\n",
              " 395,\n",
              " 396,\n",
              " 397,\n",
              " 398,\n",
              " 399,\n",
              " 400,\n",
              " 401,\n",
              " 402,\n",
              " 403,\n",
              " 404,\n",
              " 405,\n",
              " 406,\n",
              " 407,\n",
              " 408,\n",
              " 409,\n",
              " 410,\n",
              " 411,\n",
              " 412,\n",
              " 413,\n",
              " 414,\n",
              " 415,\n",
              " 416,\n",
              " 417,\n",
              " 418,\n",
              " 419,\n",
              " 420,\n",
              " 421,\n",
              " 422,\n",
              " 423,\n",
              " 424,\n",
              " 425,\n",
              " 426,\n",
              " 427,\n",
              " 428,\n",
              " 429,\n",
              " 430,\n",
              " 431,\n",
              " 432,\n",
              " 433,\n",
              " 434,\n",
              " 435,\n",
              " 436,\n",
              " 437,\n",
              " 438,\n",
              " 439,\n",
              " 440,\n",
              " 441,\n",
              " 442,\n",
              " 443,\n",
              " 444,\n",
              " 445,\n",
              " 446,\n",
              " 447,\n",
              " 448,\n",
              " 449,\n",
              " 450,\n",
              " 451,\n",
              " 452,\n",
              " 453,\n",
              " 454,\n",
              " 455,\n",
              " 456,\n",
              " 457,\n",
              " 458,\n",
              " 459,\n",
              " 460,\n",
              " 461,\n",
              " 462,\n",
              " 463,\n",
              " 464,\n",
              " 465,\n",
              " 466,\n",
              " 467,\n",
              " 468,\n",
              " 469,\n",
              " 470,\n",
              " 471,\n",
              " 472,\n",
              " 473,\n",
              " 474,\n",
              " 475,\n",
              " 476,\n",
              " 477,\n",
              " 478,\n",
              " 479,\n",
              " 480,\n",
              " 481,\n",
              " 482,\n",
              " 483,\n",
              " 484,\n",
              " 485,\n",
              " 486,\n",
              " 487,\n",
              " 488,\n",
              " 489,\n",
              " 490,\n",
              " 491,\n",
              " 492,\n",
              " 493,\n",
              " 494,\n",
              " 495,\n",
              " 496,\n",
              " 497,\n",
              " 498,\n",
              " 499,\n",
              " 500,\n",
              " 501,\n",
              " 502,\n",
              " 503,\n",
              " 504,\n",
              " 505,\n",
              " 506,\n",
              " 507,\n",
              " 508,\n",
              " 509,\n",
              " 510,\n",
              " 511,\n",
              " 512,\n",
              " 513,\n",
              " 514,\n",
              " 515,\n",
              " 516,\n",
              " 517,\n",
              " 518,\n",
              " 519,\n",
              " 520,\n",
              " 521,\n",
              " 522,\n",
              " 523,\n",
              " 524,\n",
              " 525,\n",
              " 526,\n",
              " 527,\n",
              " 528,\n",
              " 529,\n",
              " 530,\n",
              " 531,\n",
              " 532,\n",
              " 533,\n",
              " 534,\n",
              " 535,\n",
              " 536,\n",
              " 537,\n",
              " 538,\n",
              " 539,\n",
              " 540,\n",
              " 541,\n",
              " 542,\n",
              " 543,\n",
              " 544,\n",
              " 545,\n",
              " 546,\n",
              " 547,\n",
              " 548,\n",
              " 549,\n",
              " 550,\n",
              " 551,\n",
              " 552,\n",
              " 553,\n",
              " 554,\n",
              " 555,\n",
              " 556,\n",
              " 557,\n",
              " 558,\n",
              " 559,\n",
              " 560,\n",
              " 561,\n",
              " 562,\n",
              " 563,\n",
              " 564,\n",
              " 565,\n",
              " 566,\n",
              " 567,\n",
              " 568,\n",
              " 569,\n",
              " 570,\n",
              " 571,\n",
              " 572,\n",
              " 573,\n",
              " 574,\n",
              " 575,\n",
              " 576,\n",
              " 577,\n",
              " 578,\n",
              " 579,\n",
              " 580,\n",
              " 581,\n",
              " 582,\n",
              " 583,\n",
              " 584,\n",
              " 585,\n",
              " 586,\n",
              " 587,\n",
              " 588,\n",
              " 589,\n",
              " 590,\n",
              " 591,\n",
              " 592,\n",
              " 593,\n",
              " 594,\n",
              " 595,\n",
              " 596,\n",
              " 597,\n",
              " 598,\n",
              " 599,\n",
              " 600,\n",
              " 601,\n",
              " 602,\n",
              " 603,\n",
              " 604,\n",
              " 605,\n",
              " 606,\n",
              " 607,\n",
              " 608,\n",
              " 609,\n",
              " 610,\n",
              " 611,\n",
              " 612,\n",
              " 613,\n",
              " 614,\n",
              " 615,\n",
              " 616,\n",
              " 617,\n",
              " 618,\n",
              " 619,\n",
              " 620,\n",
              " 621,\n",
              " 622,\n",
              " 623,\n",
              " 624,\n",
              " 625,\n",
              " 626,\n",
              " 627,\n",
              " 628,\n",
              " 629,\n",
              " 630,\n",
              " 631,\n",
              " 632,\n",
              " 633,\n",
              " 634,\n",
              " 635,\n",
              " 636,\n",
              " 637,\n",
              " 638,\n",
              " 639,\n",
              " 640,\n",
              " 641,\n",
              " 642,\n",
              " 643,\n",
              " 644,\n",
              " 645,\n",
              " 646,\n",
              " 647,\n",
              " 648,\n",
              " 649,\n",
              " 650,\n",
              " 651,\n",
              " 652,\n",
              " 653,\n",
              " 654,\n",
              " 655,\n",
              " 656,\n",
              " 657,\n",
              " 658,\n",
              " 659,\n",
              " 660,\n",
              " 661,\n",
              " 662,\n",
              " 663,\n",
              " 664,\n",
              " 665,\n",
              " 666,\n",
              " 667,\n",
              " 668,\n",
              " 669,\n",
              " 670,\n",
              " 671,\n",
              " 672,\n",
              " 673,\n",
              " 674,\n",
              " 675,\n",
              " 676,\n",
              " 677,\n",
              " 678,\n",
              " 679,\n",
              " 680,\n",
              " 681,\n",
              " 682,\n",
              " 683,\n",
              " 684,\n",
              " 685,\n",
              " 686,\n",
              " 687,\n",
              " 688,\n",
              " 689,\n",
              " 690,\n",
              " 691,\n",
              " 692,\n",
              " 693,\n",
              " 694,\n",
              " 695,\n",
              " 696,\n",
              " 697,\n",
              " 698,\n",
              " 699,\n",
              " 700,\n",
              " 701,\n",
              " 702,\n",
              " 703,\n",
              " 704,\n",
              " 705,\n",
              " 706,\n",
              " 707,\n",
              " 708,\n",
              " 709,\n",
              " 710,\n",
              " 711,\n",
              " 712,\n",
              " 713,\n",
              " 714,\n",
              " 715,\n",
              " 716,\n",
              " 717,\n",
              " 718,\n",
              " 719,\n",
              " 720,\n",
              " 721,\n",
              " 722,\n",
              " 723,\n",
              " 724,\n",
              " 725,\n",
              " 726,\n",
              " 727,\n",
              " 728,\n",
              " 729,\n",
              " 730,\n",
              " 731,\n",
              " 732,\n",
              " 733,\n",
              " 734,\n",
              " 735,\n",
              " 736,\n",
              " 737,\n",
              " 738,\n",
              " 739,\n",
              " 740,\n",
              " 741,\n",
              " 742,\n",
              " 743,\n",
              " 744,\n",
              " 745,\n",
              " 746,\n",
              " 747,\n",
              " 748,\n",
              " 749]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "losses = []\n",
        "steps = []\n",
        "step = 0\n",
        "# 按batch输入data，train了bert之后，算predict和actual直接的loss/差距, batch_loss回传\n",
        "# 清除gradient，把bert的parameter gradient max1，update model's weights\n",
        "\n",
        "#fine-tuning a BERT model\n",
        "for epoch_num in range(EPOCHS):\n",
        "    #整个过程loop/eponch 3次，\n",
        "    bert_clf.train() #This puts the model in training mode. Unlike evaluation mode\n",
        "    #training mode allows for certain behaviors like dropout and batch normalization to work in the mode appropriate for training.\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        # #\n",
        "        #     (tensor([[ 101, 1999, 2026,  ...,    0,    0,    0],\n",
        "        #              [ 101, 2009, 2109,  ...,    0,    0,    0],\n",
        "        #              [ 101, 1045, 2074,  ...,    0,    0,    0],\n",
        "        #              [ 101, 2096, 1045,  ...,    0,    0,    0]], device='cuda:0'),\n",
        "        #      tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
        "        #              [1., 1., 1.,  ..., 0., 0., 0.],\n",
        "        #              [1., 1., 1.,  ..., 0., 0., 0.],\n",
        "        #              [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
        "        #      tensor([[1.],\n",
        "        #              [0.],\n",
        "        #              [1.],\n",
        "        #              [0.]], device='cuda:0'))\n",
        "        probas = bert_clf(token_ids, masks) #只放3个tensor的2个；train model之后得到prediction；\n",
        "        # out = tensor([[0.1845],\n",
        "        #[0.1592],\n",
        "        #[0.0929],\n",
        "        #[0.1595]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
        "\n",
        "        # 'labels= '\n",
        "        # tensor([[1.],\n",
        "        #         [1.],\n",
        "        #         [1.],\n",
        "        #         [0.]], device='cuda:0')\n",
        "        batch_loss = loss_func(probas, labels) #loss btw predicted probabilities and the actual labels.\n",
        "        # tensor(0.6197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
        "        train_loss += batch_loss.item() #取里面的0.6196919083595276\n",
        "\n",
        "        bert_clf.zero_grad() #Before you calculate the gradients for a batch, you need to zero out the gradients from the previous batch.\n",
        "        batch_loss.backward() # Performs gradient backpropagation\n",
        "        #compute the gradient of the loss with respect to all parameters of the model that are trainable.\n",
        "        #These gradients are used in the optimization step to update the parameters.\n",
        "\n",
        "        #Clamps the gradients of the model parameters to a maximum norm of 1.0 to prevent exploding gradients (a common issue in deep learning).\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step() #updates the model's weights\n",
        "\n",
        "        #Prints out the progress of training and accumulates the loss for analysis.\n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        #1000条data，batch_size是分4次做，一次是250条data，step_num=现在的batch num, avg_train_loss每次预测的差距，\n",
        "        print(\"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "        losses.append(batch_loss.item())\n",
        "        steps.append(step)\n",
        "        step += 1\n",
        "\n",
        "    display(\"train_data \",losses)\n",
        "    display(\"steps \",steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOjBnphBnTEp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "#Defines the sigmoid activation function, which is used to map the logits (outputs of the linear layer before activation)\n",
        "# to probabilities (between 0 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        numpy_probas = probas.cpu().detach().numpy()\n",
        "\n",
        "        bert_predicted += list(sigmoid(numpy_probas[:, 0]) > 0.5)"
      ],
      "metadata": {
        "id": "ESewITvKWG2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUAyHivxlv2F",
        "tags": [],
        "outputId": "c3629e88-7515-4e6d-dbce-65f095bc0bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 40 s, sys: 62 ms, total: 40.1 s\n",
            "Wall time: 40.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 测试bert\n",
        "\n",
        "bert_clf.eval() #The model is set to evaluation mode, which disables dropout layers and batch normalization during inference.\n",
        "\n",
        "#For each batch, it computes the probabilities,\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "#It loops over the test_dataloader without computing gradients (torch.no_grad()) as it's not necessary during evaluation.\n",
        "with torch.no_grad():\n",
        "\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        # result\n",
        "        # tensor([[ 5.5060],\n",
        "        # [-5.4293],\n",
        "        # [-5.5896],\n",
        "        # [ 5.4942]], device='cuda:0')\n",
        "        numpy_probas = probas.cpu().detach().numpy()\n",
        "        # converts tensor to a NumPy array,\n",
        "           #  array([[ 5.5060115],\n",
        "           # [-5.429266 ],\n",
        "           # [-5.589601 ],\n",
        "           # [ 5.494161 ]], dtype=float32)\n",
        "\n",
        "        bert_predicted += list(sigmoid(numpy_probas[:, 0]) > 0.5)\n",
        "        #bert_predicted += list(sigmoid(numpy_probas[:, 0]) )\n",
        "        # applies the sigmoid function, and determines if the probability is greater than 0.5 to\n",
        "        #  decide the predicted class. The results are accumulated into bert_predicted.\n",
        "\n",
        "        # display(numpy_probas[:, 0]) #array([ True, False, False,  True]); 取all rows but 1st column；pivot column to row\n",
        "        #display(sigmoid(numpy_probas[:, 0]) ) #array([0.99595416, 0.00436716, 0.00372261, 0.9959061 ], dtype=float32) #把predict btw(1,-1)\n",
        "        # display(sigmoid(numpy_probas[:, 0]) > 0.5) #array([ True, False, False,  True]) 大于0.5的true\n",
        "        # display(bert_predicted) # [True, False, False, True, False, True, True, True, True, True, False, False] 收集所有result\n",
        "\n",
        "        # break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_predicted[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjnCACDLOS9A",
        "outputId": "40187d4d-537a-40f5-e8f6-e1754a0915b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, False, False, True, False]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvx3QV-BnW-K",
        "outputId": "225579e0-afb4-4615-a4bc-fab35a42dbf1",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.86      0.93      0.89       522\n",
            "        True       0.91      0.83      0.87       478\n",
            "\n",
            "    accuracy                           0.88      1000\n",
            "   macro avg       0.88      0.88      0.88      1000\n",
            "weighted avg       0.88      0.88      0.88      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_y, bert_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uIOQine2kHM"
      },
      "source": [
        "Much much better! As I said — Magic :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcLKXKTLoXhX"
      },
      "source": [
        "# Distilling BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Du8ZRgoaNX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_dataset_for_distill = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_dataloader_for_distill = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1WAdqpgodhQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "bert_clf.eval()\n",
        "train_logits = []\n",
        "with torch.no_grad():\n",
        "    #tells PyTorch not to calculate gradients during the subsequent operations. This saves memory and computations\n",
        "    for step_num, batch_data in enumerate(train_dataloader_for_distill):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        # tensor([[5.4904],\n",
        "        # [5.5538],\n",
        "        # [5.5504],\n",
        "        # [5.5944]], device='cuda:0')\n",
        "        numpy_logits = logits.cpu().detach().numpy() #tensor变成array\n",
        "\n",
        "        train_logits.append(numpy_logits)\n",
        "        # if step_num >2:\n",
        "        #     display(train_logits)\n",
        "        #     break\n",
        "train_logits = np.vstack(train_logits)\n",
        "# display(train_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "_sILuifVooXo",
        "outputId": "c0af27fd-8cd8-4bc6-d11a-268b1ea69689",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.98      0.97      0.98       511\n",
            "        True       0.97      0.98      0.98       489\n",
            "\n",
            "    accuracy                           0.98      1000\n",
            "   macro avg       0.98      0.98      0.98      1000\n",
            "weighted avg       0.98      0.98      0.98      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(train_y, sigmoid(train_logits[:, 0]) > 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WnHVOCN92kHN",
        "outputId": "0f69505a-3f48-4f9c-b27f-e00fd697f14b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 5.490407 ],\n",
              "       [ 5.5537972],\n",
              "       [ 5.5504026],\n",
              "       [ 5.594379 ],\n",
              "       [ 4.5321407],\n",
              "       [ 5.232877 ],\n",
              "       [ 5.3224754],\n",
              "       [ 5.5728335],\n",
              "       [-5.3254294],\n",
              "       [-5.4204583],\n",
              "       [-5.4548507],\n",
              "       [-5.4243946],\n",
              "       [ 5.3226986],\n",
              "       [-5.4491897],\n",
              "       [ 5.2757034],\n",
              "       [-5.4687033],\n",
              "       [-5.3475957],\n",
              "       [-5.4602137],\n",
              "       [-5.587367 ],\n",
              "       [ 5.449318 ]], dtype=float32)"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_logits[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg53IDcro43h",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIVreo4FpUyi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "distilled_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LinearRegression()).fit(train_texts, train_logits)\n",
        "#和一般的linear regression不一样的是把train_logits是bert预测好的给linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88LzDz1Rqe8V",
        "tags": []
      },
      "outputs": [],
      "source": [
        "distilled_predicted_logits = distilled_model.predict(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "OIKbicwtqlMI",
        "outputId": "698d4486-43a5-4bef-b032-7461ff6fbac5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.84      0.76      0.80       522\n",
            "        True       0.76      0.84      0.80       478\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.80      0.80      0.80      1000\n",
            "weighted avg       0.80      0.80      0.80      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_y, sigmoid(distilled_predicted_logits[:, 0]) > 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "lv77OCGNq0vx",
        "outputId": "a6934d63-6a1f-4f6c-f40d-97ac0cb582e4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([496.,   5.,   3.,   2.,   4.,   0.,   1.,   2.,   4., 483.]),\n",
              " array([-5.63385344, -4.50565815, -3.37746334, -2.24926829, -1.12107325,\n",
              "         0.0071218 ,  1.13531685,  2.2635119 ,  3.39170694,  4.51990223,\n",
              "         5.64809704]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(train_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "byCQEPCj2kHP"
      },
      "source": [
        "*预测的结果*-5 到 5之间 ，之前是0和1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6dilhxX2kHP"
      },
      "source": [
        "\\\n",
        "现在用其他的数据试\n",
        "Now to the interesting part, we use the unlabeled set and “label” it using our fine-tuned BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39Bbw5WBuUyo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_data = train_data_full[1000:6000] ## We use another 5000 reviews as unlabeled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJewqAoHfz3i",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_texts, _ = list(zip(*map(lambda d: (d['text'], d['sentiment']), unlabeled_data))) #分开 x 和 Y ;Y = —_这里不要了，"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd8HedTBf4QA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], unlabeled_texts)) #tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJxwmpfPgIm8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, unlabeled_tokens)) #找token id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dSf3ec9dgMhe",
        "outputId": "d5a8ea93-d8c3-4c7a-d117-3bc85ffe5def",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 512)"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unlabeled_tokens_ids = tf.keras.utils.pad_sequences(unlabeled_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "unlabeled_tokens_ids.shape #token的长度填充+截断，"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60TsakNUghH6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_masks = [[float(i > 0) for i in ii] for ii in unlabeled_tokens_ids] #自己找 attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3n7N48egm95",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_tokens_tensor = torch.tensor(unlabeled_tokens_ids) #把token id , attention mask 放到tensor\n",
        "unlabeled_masks_tensor = torch.tensor(unlabeled_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqpH5oRXgSOr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## We \"predict\" the raw logits using the fine-tuned BERT model\n",
        "\n",
        "unlabeled_dataset = TensorDataset(unlabeled_tokens_tensor, unlabeled_masks_tensor)\n",
        "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-7WXofDlglmX",
        "outputId": "c5d31588-5e27-42e8-8744-01aa2aec6e1f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1249/1250.0\n"
          ]
        }
      ],
      "source": [
        "bert_clf.eval()\n",
        "unlabeled_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(unlabeled_dataloader): #5000条，4batch，每次1250\n",
        "\n",
        "        token_ids, masks = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "\n",
        "        unlabeled_logits.append(numpy_logits)\n",
        "        clear_output(wait=True)\n",
        "        print(\"{0}/{1}\".format(step_num, len(unlabeled_data) / BATCH_SIZE))\n",
        "unlabeled_logits = np.vstack(unlabeled_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "e9Dtb46C2kHR",
        "outputId": "8a2dffb3-a4c4-4d26-a67a-468f1054b4dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-5.507926 ],\n",
              "       [ 5.5109105],\n",
              "       [ 5.5285363],\n",
              "       [-4.888447 ],\n",
              "       [-5.477286 ],\n",
              "       [-5.290694 ],\n",
              "       [-5.414143 ],\n",
              "       [-5.471599 ],\n",
              "       [ 5.6312623],\n",
              "       [-5.184795 ],\n",
              "       [ 5.363724 ],\n",
              "       [-5.5648894],\n",
              "       [-5.1566586],\n",
              "       [ 5.502614 ],\n",
              "       [-5.3328004],\n",
              "       [ 5.4390597],\n",
              "       [-5.3108563],\n",
              "       [-5.573649 ],\n",
              "       [-5.331365 ],\n",
              "       [ 5.07082  ]], dtype=float32)"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unlabeled_logits[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px_qoa5N2kHR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfoRXLQchGZq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabeled_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LinearRegression()).fit(unlabeled_texts, unlabeled_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RhdIEXliy3B",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unlabele_predicted_logits = unlabeled_model.predict(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "ytR5XSE-i3kK",
        "outputId": "02f3e0e9-6be5-454a-fbc1-f23b97de8902",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.87      0.84      0.85       522\n",
            "        True       0.83      0.86      0.84       478\n",
            "\n",
            "    accuracy                           0.85      1000\n",
            "   macro avg       0.85      0.85      0.85      1000\n",
            "weighted avg       0.85      0.85      0.85      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_y, sigmoid(unlabele_predicted_logits[:, 0]) > 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nOh6uzoi63N"
      },
      "source": [
        "Not as great as the original fine-tuned BERT, but it’s much better than the baseline! Now we are ready to deploy this small model to production and enjoy both good quality and inference speed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rbdNZJo2kHS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m120",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}